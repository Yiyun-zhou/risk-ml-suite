import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import shap
import numpy as np
#from src.drift import psi_report

from pathlib import Path
import sys, json, joblib, pandas as pd
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))
model = joblib.load(ROOT / "models" / "lgbm_kaggle.joblib")
feature_cols = json.loads((ROOT/"models"/"kaggle_features.json").read_text())


# è½½å…¥æ•°æ® & æ¨¡å‹
train =  pd.read_csv("data/processed/train.csv", parse_dates=["event_time"])
valid = pd.read_csv("data/processed/valid.csv", parse_dates=["event_time"])
test  = pd.read_csv("data/processed/test.csv",  parse_dates=["event_time"])

st.set_page_config(page_title="Risk ML Dashboard", layout="wide")


MODEL_PATH = ROOT / "models" / "lgbm_kaggle.joblib"

def unwrap_pipeline(model):
    prep, clf = None, model
    if hasattr(model, "named_steps"):
        prep = model.named_steps.get("prep", None)
        clf = model.named_steps.get("clf", model)
    if hasattr(clf, "base_estimator_"):
        base = getattr(clf, "base_estimator_", None)
        if base is not None:
            clf = base
    return prep, clf

@st.cache_data(show_spinner=False)  # å…³é”®ï¼šç¼“å­˜æŒ‰ model_path + n_samples + feature_cols å†…å®¹
def compute_shap_from_path(model_path_str, df, feature_cols, n_samples=400):
    model = joblib.load(model_path_str)  # åœ¨ç¼“å­˜å‡½æ•°å†…éƒ¨åŠ è½½
    prep, clf = unwrap_pipeline(model)

    X_df = df[feature_cols].sample(min(n_samples, len(df)), random_state=42)
    X_used = prep.transform(X_df) if prep is not None else X_df.values

    try:
        import lightgbm as lgb, xgboost as xgb
        if isinstance(clf, (lgb.LGBMClassifier, xgb.XGBClassifier)):
            explainer = shap.TreeExplainer(clf)
            sv = explainer.shap_values(X_used)
            if isinstance(sv, list): sv = sv[1]
            exp_value = explainer.expected_value
            if isinstance(exp_value, (list, np.ndarray)):
                exp_value = exp_value[1]
            model_type = "tree"
            return sv, X_used, float(exp_value), model_type
    except Exception:
        pass

    from sklearn.linear_model import LogisticRegression
    if isinstance(clf, LogisticRegression):
        explainer = shap.LinearExplainer(clf, X_used)
        sv = explainer.shap_values(X_used)
        exp_value = explainer.expected_value
        return sv, X_used, float(np.array(exp_value).mean()), "linear"

    # å…œåº•ï¼špermutationï¼ˆå¯¹æ•´ä¸ª pipelineï¼‰
    f = lambda X_df_input: model.predict_proba(X_df_input)[:, 1]
    bg = df[feature_cols].sample(min(200, len(df)), random_state=0)
    explainer = shap.Explainer(f, bg, algorithm="permutation")
    ex = explainer(X_df)
    sv = np.array(ex.values)
    exp_value = float(np.array(ex.base_values).mean())
    return sv, X_df.values, exp_value, "perm"



# --- é¡µé¢é€‰æ‹© ---
menu = ["KPI æ€»è§ˆ", "é˜ˆå€¼ä¼˜åŒ–", "æ¼‚ç§»ç›‘æ§", "æ¨¡å‹è§£é‡Š"]
choice = st.sidebar.radio("é€‰æ‹©", menu)

# --- 1. KPI æ€»è§ˆ ---
if choice == "KPI æ€»è§ˆ":
    st.header("ğŸ“Š KPI æ€»è§ˆ")
    fraud_rate = test["Class"].mean()
    st.metric("Fraud Rate", f"{fraud_rate:.3%}")

    # å‡è®¾æˆ‘ä»¬åœ¨ reports/drift_metrics_hourly.csv é‡Œæœ‰ rolling æŒ‡æ ‡
    metrics = pd.read_csv("reports/drift_metrics.csv")
    st.line_chart(metrics.set_index("hour")[["auc_pr", "roc_auc"]])

# --- 2. é˜ˆå€¼ä¼˜åŒ– ---
elif choice == "é˜ˆå€¼ä¼˜åŒ–":
    st.header("âš–ï¸ é˜ˆå€¼ä¼˜åŒ–")
    from sklearn.metrics import precision_recall_fscore_support

    X = test[feature_cols]
    proba = model.predict_proba(X)[:, 1]
    
    X, y = test[feature_cols], test["Class"]
    proba = model.predict_proba(X)[:,1]

    thr = st.slider("é€‰æ‹©é˜ˆå€¼", 0.0, 1.0, 0.5, 0.01)
    pred = (proba >= thr).astype(int)
    p, r, f, _ = precision_recall_fscore_support(y, pred, average="binary")

    st.write(f"Precision={p:.3f} | Recall={r:.3f} | F1={f:.3f}")

    fig, ax = plt.subplots()
    ax.hist(proba, bins=50, alpha=0.7)
    ax.axvline(thr, color="red", linestyle="--", label=f"Threshold={thr}")
    ax.legend()
    st.pyplot(fig)

# --- 3. æ¼‚ç§»ç›‘æ§ ---
elif choice == "æ¼‚ç§»ç›‘æ§":
    st.header("ğŸ“ˆ ç‰¹å¾æ¼‚ç§»ç›‘æ§")
    train = pd.read_csv("data/processed/train.csv", parse_dates=["event_time"])
    psi_df = pd.read_csv("reports/PSI_feature_metrics.csv")

    st.dataframe(psi_df)
    fig, ax = plt.subplots(figsize=(8,5))
    ax.barh(psi_df["feature"], psi_df["psi"], color="steelblue")
    ax.axvline(0.1, color="orange", linestyle="--")
    ax.axvline(0.25, color="red", linestyle="--")
    st.pyplot(fig)

# --- 4. æ¨¡å‹è§£é‡Š (SHAP) ---
elif choice == "æ¨¡å‹è§£é‡Š":
    st.header("ğŸ” æ¨¡å‹è§£é‡Š (SHAP)")

    # è·¯å¾„
    SHAP_DIR = ROOT / "reports"
    FIG_DIR  = SHAP_DIR / "figures"
    # è¯» meta / values / X
    import json, numpy as np, pandas as pd, matplotlib.pyplot as plt

    sv_path   = SHAP_DIR / "shap_values_sample.npy"

    shap_values = np.load(sv_path)              # [N, d]
   
    # === å…¨å±€å›¾ï¼šç›´æ¥å±•ç¤ºå·²ç”Ÿæˆçš„å›¾ç‰‡ ===
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("å…¨å±€é‡è¦æ€§ï¼ˆBarï¼‰")
        st.image(str(FIG_DIR / "shap_summary_bar.png"), use_column_width=True)
    with col2:
        st.subheader("å…¨å±€åˆ†å¸ƒï¼ˆDotï¼‰")
        st.image(str(FIG_DIR / "shap_summary_dot.png"), use_column_width=True)

    # === å±€éƒ¨è§£é‡Šå‡½æ•°ï¼šTop-K æ¡å½¢å›¾ ===
    def plot_local_explanation(sv_row: np.ndarray, feature_cols, k=8, base=0.0):
        top_idx = np.argsort(np.abs(sv_row))[::-1][:k]
        feats = np.array(feature_cols)[top_idx]
        vals  = sv_row[top_idx]
        fig = plt.figure(figsize=(6, 4))
        colors = ["tab:red" if v > 0 else "tab:blue" for v in vals]
        plt.barh(feats[::-1], vals[::-1], color=colors[::-1])
        plt.axvline(0, color="k", linewidth=0.8)
        plt.title(f"Top-{k} features(base={base:.3f})")
        plt.tight_layout()
        return fig

    st.subheader("å±€éƒ¨è§£é‡Šï¼ˆå•ç¬”äº¤æ˜“ï¼‰")
    k = st.slider("Top-K ç‰¹å¾", 3, 20, 8, 1)
    idx_local = st.number_input("é€‰æ‹©æ ·æœ¬ indexï¼ˆ0 ~ N-1ï¼‰", min_value=0, max_value=len(shap_values)-1, value=0)

    sv_i = shap_values[idx_local]
    fig = plot_local_explanation(sv_i, feature_cols, k=k)
    st.pyplot(fig, clear_figure=True)
